{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Session 4.Vision-Language Model (VLM) Prompt Tuning\n",
        "\n",
        "## Contents\n",
        "[1] Preparation \\\n",
        "[2] Load pre-trained CLIP Model \\\n",
        "[3] CoOpCLIP Implementation \\\n",
        "[4] CoOpCLIP Training\n",
        "\n",
        "\n",
        "## References\n",
        "- Learning to Prompt for Vision-Language Models (CoOp): https://github.com/KaiyangZhou/CoOp\n",
        "- Prompt Learning via Meta-Regularization (ProMetaR): https://github.com/mlvlab/ProMetaR"
      ],
      "metadata": {
        "id": "SlXBcE5Jyasd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# [1] Preparation"
      ],
      "metadata": {
        "id": "33ucm40jy2ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Clone github repository"
      ],
      "metadata": {
        "id": "0k0iyRGO3nxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mlvlab/ProMetaR.git"
      ],
      "metadata": {
        "id": "p_N1HeFoj5Me",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f802ad-fadd-4f28-f8a2-c06577c117e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ProMetaR' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It will make ProMetaR folder on left side."
      ],
      "metadata": {
        "id": "TbtCGMbbKR37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Install Requirements"
      ],
      "metadata": {
        "id": "HZirgCYRzAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ProMetaR/\n",
        "\n",
        "!git clone https://github.com/KaiyangZhou/Dassl.pytorch.git\n",
        "%cd Dassl.pytorch/\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -r requirements.txt\n",
        "!cp -r dassl ../\n",
        "# Install this library (no need to re-build if the source code is modified)\n",
        "# !python setup.py develop\n",
        "%cd ..\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "MsVCEqXpzBjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- *If an error occurs, click ‘Run Session Again’ and then restart the runtime from the beginning.*"
      ],
      "metadata": {
        "id": "ryx2uX_0G3h6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Load Requirements and functions"
      ],
      "metadata": {
        "id": "Mm5Yl16mCUnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from clip import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import argparse\n",
        "from dassl.utils import setup_logger, set_random_seed, collect_env_info\n",
        "from dassl.config import get_cfg_default\n",
        "from dassl.engine import build_trainer\n",
        "from dassl.engine import TRAINER_REGISTRY, TrainerX\n",
        "from dassl.metrics import compute_accuracy\n",
        "from dassl.utils import load_pretrained_weights, load_checkpoint\n",
        "from dassl.optim import build_optimizer, build_lr_scheduler\n",
        "\n",
        "# custom\n",
        "import datasets.oxford_pets\n",
        "import datasets.oxford_flowers\n",
        "import datasets.fgvc_aircraft\n",
        "import datasets.dtd\n",
        "import datasets.eurosat\n",
        "import datasets.stanford_cars\n",
        "import datasets.food101\n",
        "import datasets.sun397\n",
        "import datasets.caltech101\n",
        "import datasets.ucf101\n",
        "import datasets.imagenet\n",
        "import datasets.imagenet_sketch\n",
        "import datasets.imagenetv2\n",
        "import datasets.imagenet_a\n",
        "import datasets.imagenet_r"
      ],
      "metadata": {
        "id": "Tck2BxWu17UB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_args(args, cfg):\n",
        "    print(\"***************\")\n",
        "    print(\"** Arguments **\")\n",
        "    print(\"***************\")\n",
        "    optkeys = list(args.__dict__.keys())\n",
        "    optkeys.sort()\n",
        "    for key in optkeys:\n",
        "        print(\"{}: {}\".format(key, args.__dict__[key]))\n",
        "    print(\"************\")\n",
        "    print(\"** Config **\")\n",
        "    print(\"************\")\n",
        "    print(cfg)\n",
        "\n",
        "def reset_cfg(cfg, args):\n",
        "    if args.root:\n",
        "        cfg.DATASET.ROOT = args.root\n",
        "    if args.output_dir:\n",
        "        cfg.OUTPUT_DIR = args.output_dir\n",
        "    if args.seed:\n",
        "        cfg.SEED = args.seed\n",
        "    if args.trainer:\n",
        "        cfg.TRAINER.NAME = args.trainer\n",
        "    cfg.DATASET.NUM_SHOTS = 16\n",
        "    cfg.DATASET.SUBSAMPLE_CLASSES = args.subsample_classes\n",
        "    cfg.DATALOADER.TRAIN_X.BATCH_SIZE = args.train_batch_size\n",
        "    cfg.OPTIM.MAX_EPOCH = args.epoch\n",
        "\n",
        "def extend_cfg(cfg):\n",
        "    \"\"\"\n",
        "    Add new config variables.\n",
        "    \"\"\"\n",
        "    from yacs.config import CfgNode as CN\n",
        "    cfg.TRAINER.COOP = CN()\n",
        "    cfg.TRAINER.COOP.N_CTX = 16  # number of context vectors\n",
        "    cfg.TRAINER.COOP.CSC = False  # class-specific context\n",
        "    cfg.TRAINER.COOP.CTX_INIT = \"\"  # initialization words\n",
        "    cfg.TRAINER.COOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
        "    cfg.TRAINER.COOP.CLASS_TOKEN_POSITION = \"end\"  # 'middle' or 'end' or 'front'\n",
        "    cfg.TRAINER.COCOOP = CN()\n",
        "    cfg.TRAINER.COCOOP.N_CTX = 4  # number of context vectors\n",
        "    cfg.TRAINER.COCOOP.CTX_INIT = \"a photo of a\"  # initialization words\n",
        "    cfg.TRAINER.COCOOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
        "    cfg.TRAINER.PROMETAR = CN()\n",
        "    cfg.TRAINER.PROMETAR.N_CTX_VISION = 4  # number of context vectors at the vision branch\n",
        "    cfg.TRAINER.PROMETAR.N_CTX_TEXT = 4  # number of context vectors at the language branch\n",
        "    cfg.TRAINER.PROMETAR.CTX_INIT = \"a photo of a\"  # initialization words\n",
        "    cfg.TRAINER.PROMETAR.PREC = \"fp16\"  # fp16, fp32, amp\n",
        "    cfg.TRAINER.PROMETAR.PROMPT_DEPTH_VISION = 9  # Max 12, minimum 0, for 0 it will be using shallow IVLP prompting (J=1)\n",
        "    cfg.TRAINER.PROMETAR.PROMPT_DEPTH_TEXT = 9  # Max 12, minimum 0, for 0 it will be using shallow IVLP prompting (J=1)\n",
        "    cfg.DATASET.SUBSAMPLE_CLASSES = \"all\"  # all, base or new\n",
        "    cfg.TRAINER.PROMETAR.ADAPT_LR = 0.0005\n",
        "    cfg.TRAINER.PROMETAR.LR_RATIO = 0.0005\n",
        "    cfg.TRAINER.PROMETAR.FAST_ADAPTATION = False\n",
        "    cfg.TRAINER.PROMETAR.MIXUP_ALPHA = 0.5\n",
        "    cfg.TRAINER.PROMETAR.MIXUP_BETA = 0.5\n",
        "    cfg.TRAINER.PROMETAR.DIM_RATE=8\n",
        "    cfg.OPTIM_VNET = CN()\n",
        "    cfg.OPTIM_VNET.NAME = \"adam\"\n",
        "    cfg.OPTIM_VNET.LR = 0.0003\n",
        "    cfg.OPTIM_VNET.WEIGHT_DECAY = 5e-4\n",
        "    cfg.OPTIM_VNET.MOMENTUM = 0.9\n",
        "    cfg.OPTIM_VNET.SGD_DAMPNING = 0\n",
        "    cfg.OPTIM_VNET.SGD_NESTEROV = False\n",
        "    cfg.OPTIM_VNET.RMSPROP_ALPHA = 0.99\n",
        "    cfg.OPTIM_VNET.ADAM_BETA1 = 0.9\n",
        "    cfg.OPTIM_VNET.ADAM_BETA2 = 0.999\n",
        "    cfg.OPTIM_VNET.STAGED_LR = False\n",
        "    cfg.OPTIM_VNET.NEW_LAYERS = ()\n",
        "    cfg.OPTIM_VNET.BASE_LR_MULT = 0.1\n",
        "    # Learning rate scheduler\n",
        "    cfg.OPTIM_VNET.LR_SCHEDULER = \"single_step\"\n",
        "    # -1 or 0 means the stepsize is equal to max_epoch\n",
        "    cfg.OPTIM_VNET.STEPSIZE = (-1, )\n",
        "    cfg.OPTIM_VNET.GAMMA = 0.1\n",
        "    cfg.OPTIM_VNET.MAX_EPOCH = 10\n",
        "    # Set WARMUP_EPOCH larger than 0 to activate warmup training\n",
        "    cfg.OPTIM_VNET.WARMUP_EPOCH = -1\n",
        "    # Either linear or constant\n",
        "    cfg.OPTIM_VNET.WARMUP_TYPE = \"linear\"\n",
        "    # Constant learning rate when type=constant\n",
        "    cfg.OPTIM_VNET.WARMUP_CONS_LR = 1e-5\n",
        "    # Minimum learning rate when type=linear\n",
        "    cfg.OPTIM_VNET.WARMUP_MIN_LR = 1e-5\n",
        "    # Recount epoch for the next scheduler (last_epoch=-1)\n",
        "    # Otherwise last_epoch=warmup_epoch\n",
        "    cfg.OPTIM_VNET.WARMUP_RECOUNT = True\n",
        "\n",
        "def setup_cfg(args):\n",
        "    cfg = get_cfg_default()\n",
        "    extend_cfg(cfg)\n",
        "    # 1. From the dataset config file\n",
        "    if args.dataset_config_file:\n",
        "        cfg.merge_from_file(args.dataset_config_file)\n",
        "    # 2. From the method config file\n",
        "    if args.config_file:\n",
        "        cfg.merge_from_file(args.config_file)\n",
        "    # 3. From input arguments\n",
        "    reset_cfg(cfg, args)\n",
        "    cfg.freeze()\n",
        "    return cfg"
      ],
      "metadata": {
        "id": "H_jVzJWQChw9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Download EuroSAT dataset"
      ],
      "metadata": {
        "id": "2quBDo0n31-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir outputs\n",
        "%mkdir data\n",
        "\n",
        "%cd data\n",
        "%mkdir eurosat\n",
        "!wget http://madm.dfki.de/files/sentinel/EuroSAT.zip EuroSAT.zip\n",
        "\n",
        "!unzip -o EuroSAT.zip -d eurosat/\n",
        "%cd eurosat\n",
        "!gdown 1Ip7yaCWFi0eaOFUGga0lUdVi_DDQth1o\n",
        "\n",
        "%cd ../../\n"
      ],
      "metadata": {
        "id": "Rq87rDm-kTjY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The structure of the data folder inside the ProMetaR folder should be as follows:\n",
        "\n",
        "\t•\teurosat/2750\n",
        "\t•\teurosat/split_zhou_EuroSAT.json"
      ],
      "metadata": {
        "id": "9xps7PVzG6qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ASnsQA9hzahu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] Load pre-trained CLIP Model"
      ],
      "metadata": {
        "id": "OOUyP2s2dDXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_tokenizer = _Tokenizer()\n",
        "\n",
        "def load_clip_to_cpu(cfg): # Load CLIP\n",
        "    backbone_name = cfg.MODEL.BACKBONE.NAME\n",
        "    url = clip._MODELS[backbone_name]\n",
        "    model_path = clip._download(url)\n",
        "\n",
        "    try:\n",
        "        # loading JIT archive\n",
        "        model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n",
        "        state_dict = None\n",
        "\n",
        "    except RuntimeError:\n",
        "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
        "\n",
        "    if cfg.TRAINER.NAME == \"\":\n",
        "      design_trainer = \"CoOp\"\n",
        "    else:\n",
        "      design_trainer = cfg.TRAINER.NAME\n",
        "    design_details = {\"trainer\": design_trainer,\n",
        "                      \"vision_depth\": 0,\n",
        "                      \"language_depth\": 0, \"vision_ctx\": 0,\n",
        "                      \"language_ctx\": 0}\n",
        "    model = clip.build_model(state_dict or model.state_dict(), design_details)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "mZlULoCU1j0j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dassl.config import get_cfg_default\n",
        "cfg = get_cfg_default()\n",
        "cfg.MODEL.BACKBONE.NAME = \"ViT-B/16\" # Set the vision encoder backbone of CLIP to ViT.\n",
        "clip_model = load_clip_to_cpu(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiyjkEkcXPyR",
        "outputId": "34462ff0-6578-4f4b-ad68-54cdac352708"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 351M/351M [00:03<00:00, 100MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check CLIP model"
      ],
      "metadata": {
        "id": "NH3Ehw3xNtZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(clip_model)"
      ],
      "metadata": {
        "id": "vYuHDRDwNxe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [3] CoOpCLIP Implementation\n",
        "\n",
        "CoOpCLIP is composed of pre-trained CLIP Text encoder, pre-trained CLIP Image encoder, and learnable prompt."
      ],
      "metadata": {
        "id": "VgsA4RzQ8yCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Module 1: CLIP Text Encoder\n",
        "\n",
        "**Input**\n",
        "- token prefix (SOS token) + learnable prompt + class label + token suffix (CLS token)\n",
        "\n",
        "**Output**\n",
        "- text feature of input prompts"
      ],
      "metadata": {
        "id": "kQY1UE9S1pcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts): # Call model forward\n",
        "        x = prompts + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "        return x"
      ],
      "metadata": {
        "id": "PPzA8a1n1oaR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Module 2: CLIP Image Encoder\n",
        "\n",
        "**Input**\n",
        "- image\n",
        "\n",
        "**Output**\n",
        "- image feature"
      ],
      "metadata": {
        "id": "Q17p2MV42W8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(clip_model.visual)"
      ],
      "metadata": {
        "id": "DAZ0nR2X2WM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Module 3: Learnable Prompt\n",
        "\n",
        "**Output**\n",
        "- Learnable prompt"
      ],
      "metadata": {
        "id": "NVL_tLqM2ELT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CoOpPromptLearner(nn.Module):\n",
        "    def __init__(self, cfg, classnames, clip_model):\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        n_ctx = cfg.TRAINER.COOP.N_CTX\n",
        "        ctx_init = cfg.TRAINER.COOP.CTX_INIT\n",
        "        dtype = clip_model.dtype\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "        cfg_imsize = cfg.INPUT.SIZE[0]\n",
        "        assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})\"\n",
        "\n",
        "        ### Learnable Prompts Initialization ###\n",
        "        if ctx_init:\n",
        "            # use given words to initialize context vectors\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
        "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            # random initialization\n",
        "            if cfg.TRAINER.COOP.CSC:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim, dtype=dtype)\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
        "            nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "        ######################################################\n",
        "        print(f'Initial context: \"{prompt_prefix}\"')\n",
        "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
        "        self.ctx = nn.Parameter(ctx_vectors)  # Wrap the initialized prompts above as parameters to make them trainable.\n",
        "\n",
        "        ### Tokenize ###\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]  # 예) \"Forest\"\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames] # 예) \"A photo of Forest.\"\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]) # 예) [49406, 320, 1125, 539...]\n",
        "        ################\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
        "        # These token vectors will be saved when in save_model(),\n",
        "        # but they should be ignored in load_model() as we want to use\n",
        "        # those computed using the current class names\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS (문장의 시작을 알려주는 토큰)\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS (문장의 끝을 알려주는 토큰)\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
        "        self.name_lens = name_lens\n",
        "\n",
        "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
        "        # dim0 is either batch_size (during training) or n_cls (during testing)\n",
        "        # ctx: context tokens, with shape of (dim0, n_ctx, ctx_dim)\n",
        "        # prefix: the sos token, with shape of (n_cls, 1, ctx_dim)\n",
        "        # suffix: remaining tokens, with shape of (n_cls, *, ctx_dim)\n",
        "        if label is not None:\n",
        "            prefix = prefix[label]\n",
        "            suffix = suffix[label]\n",
        "        prompts = torch.cat(\n",
        "            [\n",
        "                prefix,  # (dim0, 1, dim)\n",
        "                ctx,  # (dim0, n_ctx, dim)\n",
        "                suffix,  # (dim0, *, dim)\n",
        "            ],\n",
        "            dim=1,\n",
        "        )\n",
        "        return prompts\n",
        "\n",
        "    def forward(self):\n",
        "        ctx = self.ctx\n",
        "        if ctx.dim() == 2:\n",
        "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        prompts = self.construct_prompts(ctx, prefix, suffix) #[시작토큰, Input prompts,끝 토큰]\n",
        "        return prompts"
      ],
      "metadata": {
        "id": "3_YQTO0Y2R1C"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make CoOpCLIP (Module1 + Module2 + Module3)\n",
        "\n",
        "**Input**\n",
        "- Image\n",
        "\n",
        "**Output**\n",
        "- Logit\n",
        "\n",
        "**How to compute logit?**\n",
        "- image_features : Image representation $\\mathbf{f}$\n",
        "- text_features : Text representation $g\\left(\\mathbf{t}_i\\right)$\n",
        "- Logit:\n",
        "$p\\left(y=i | \\mathbf{x} \\right) = \\frac{\\exp \\left(\\cos\\left(g\\left(\\mathbf{t}_i\\right),\\mathbf{f} \\right)/\\tau \\right)}{\\sum_{j=1}^K \\exp \\left(\\cos\\left(g\\left(\\mathbf{t}_j\\right),\\mathbf{f} \\right)/\\tau \\right)} $"
      ],
      "metadata": {
        "id": "Howh4jCR-Mst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CoOpCustomCLIP(nn.Module):\n",
        "    def __init__(self, cfg, classnames, clip_model):\n",
        "        super().__init__()\n",
        "        self.prompt_learner = CoOpPromptLearner(cfg, classnames, clip_model)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, image):\n",
        "        image_features = self.image_encoder(image.type(self.dtype))\n",
        "\n",
        "        prompts = self.prompt_learner()\n",
        "        tokenized_prompts = self.tokenized_prompts\n",
        "        text_features = self.text_encoder(prompts, tokenized_prompts)\n",
        "\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits = logit_scale * image_features @ text_features.t()\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "qHIcuAkF-VHX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [4] CoOpCLIP Training"
      ],
      "metadata": {
        "id": "95g7yRCT4brR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training configurations"
      ],
      "metadata": {
        "id": "mXo4-zJJDOYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--root\", type=str, default=\"data/\", help=\"path to dataset\")\n",
        "parser.add_argument(\"--output-dir\", type=str, default=\"outputs/cocoop3\", help=\"output directory\")\n",
        "parser.add_argument(\n",
        "    \"--seed\", type=int, default=1, help=\"only positive value enables a fixed seed\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--config-file\", type=str, default=\"configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\", help=\"path to config file\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--dataset-config-file\",\n",
        "    type=str,\n",
        "    default=\"configs/datasets/eurosat.yaml\",\n",
        "    help=\"path to config file for dataset setup\",\n",
        ")\n",
        "parser.add_argument(\"--trainer\", type=str, default=\"CoOp\", help=\"name of trainer\")\n",
        "parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"evaluation only\")\n",
        "parser.add_argument(\n",
        "    \"--model-dir\",\n",
        "    type=str,\n",
        "    default=\"\",\n",
        "    help=\"load model from this directory for eval-only mode\",\n",
        ")\n",
        "parser.add_argument(\"--train-batch-size\", type=int, default=4)\n",
        "parser.add_argument(\"--epoch\", type=int, default=10)\n",
        "parser.add_argument(\"--subsample-classes\", type=str, default=\"base\")\n",
        "parser.add_argument(\n",
        "    \"--load-epoch\", type=int, default=0, help=\"load model weights at this epoch for evaluation\"\n",
        ")\n",
        "args = parser.parse_args([])"
      ],
      "metadata": {
        "id": "ZcNsq17FDRzk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trainer Class"
      ],
      "metadata": {
        "id": "yB0_2nm0C73P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@TRAINER_REGISTRY.register(force=True)\n",
        "class CoOp(TrainerX):\n",
        "    \"\"\"Context Optimization (CoOp).\n",
        "\n",
        "    Learning to Prompt for Vision-Language Models\n",
        "    https://arxiv.org/abs/2109.01134\n",
        "    \"\"\"\n",
        "\n",
        "    def check_cfg(self, cfg):\n",
        "        assert cfg.TRAINER.COOP.PREC in [\"fp16\", \"fp32\", \"amp\"]\n",
        "\n",
        "    def build_model(self):\n",
        "        cfg = self.cfg\n",
        "        classnames = self.dm.dataset.classnames\n",
        "\n",
        "        print(f\"Loading CLIP (backbone: {cfg.MODEL.BACKBONE.NAME})\")\n",
        "        clip_model = load_clip_to_cpu(cfg)\n",
        "\n",
        "        if cfg.TRAINER.COOP.PREC == \"fp32\" or cfg.TRAINER.COOP.PREC == \"amp\":\n",
        "            # CLIP's default precision is fp16\n",
        "            clip_model.float()\n",
        "\n",
        "        print(\"Building custom CLIP\")\n",
        "        self.model = CoOpCustomCLIP(cfg, classnames, clip_model)\n",
        "\n",
        "        print(\"Turning off gradients in both the image and the text encoder\")\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if \"prompt_learner\" not in name:\n",
        "                param.requires_grad_(False)\n",
        "\n",
        "        if cfg.MODEL.INIT_WEIGHTS:\n",
        "            load_pretrained_weights(self.model.prompt_learner, cfg.MODEL.INIT_WEIGHTS)\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        # NOTE: only give prompt_learner to the optimizer\n",
        "        self.optim = build_optimizer(self.model.prompt_learner, cfg.OPTIM)\n",
        "        self.sched = build_lr_scheduler(self.optim, cfg.OPTIM)\n",
        "        self.register_model(\"prompt_learner\", self.model.prompt_learner, self.optim, self.sched)\n",
        "\n",
        "        self.scaler = GradScaler() if cfg.TRAINER.COOP.PREC == \"amp\" else None\n",
        "\n",
        "        # Note that multi-gpu training could be slow because CLIP's size is\n",
        "        # big, which slows down the copy operation in DataParallel\n",
        "        device_count = torch.cuda.device_count()\n",
        "        if device_count > 1:\n",
        "            print(f\"Multiple GPUs detected (n_gpus={device_count}), use all of them!\")\n",
        "            self.model = nn.DataParallel(self.model)\n",
        "\n",
        "    def before_train(self):\n",
        "        directory = self.cfg.OUTPUT_DIR\n",
        "        if self.cfg.RESUME:\n",
        "            directory = self.cfg.RESUME\n",
        "        self.start_epoch = self.resume_model_if_exist(directory)\n",
        "\n",
        "        # Remember the starting time (for computing the elapsed time)\n",
        "        self.time_start = time.time()\n",
        "\n",
        "    def forward_backward(self, batch):\n",
        "        image, label = self.parse_batch_train(batch)\n",
        "\n",
        "        prec = self.cfg.TRAINER.COOP.PREC\n",
        "        output = self.model(image)      # Input image 모델 통과\n",
        "        loss = F.cross_entropy(output, label)  # Loss 선언\n",
        "        self.model_backward_and_update(loss)  # Backward 및 모델 parameter 업데이트\n",
        "\n",
        "        loss_summary = {\n",
        "            \"loss\": loss.item(),\n",
        "            \"acc\": compute_accuracy(output, label)[0].item(),\n",
        "        }\n",
        "\n",
        "        if (self.batch_idx + 1) == self.num_batches:\n",
        "            self.update_lr()\n",
        "\n",
        "        return loss_summary\n",
        "\n",
        "    def parse_batch_train(self, batch):\n",
        "        input = batch[\"img\"]\n",
        "        label = batch[\"label\"]\n",
        "        input = input.to(self.device)\n",
        "        label = label.to(self.device)\n",
        "        return input, label\n",
        "\n",
        "    def load_model(self, directory, epoch=None):\n",
        "        if not directory:\n",
        "            print(\"Note that load_model() is skipped as no pretrained model is given\")\n",
        "            return\n",
        "\n",
        "        names = self.get_model_names()\n",
        "\n",
        "        # By default, the best model is loaded\n",
        "        model_file = \"model-best.pth.tar\"\n",
        "\n",
        "        if epoch is not None:\n",
        "            model_file = \"model.pth.tar-\" + str(epoch)\n",
        "\n",
        "        for name in names:\n",
        "            model_path = osp.join(directory, name, model_file)\n",
        "\n",
        "            if not osp.exists(model_path):\n",
        "                raise FileNotFoundError('Model not found at \"{}\"'.format(model_path))\n",
        "\n",
        "            checkpoint = load_checkpoint(model_path)\n",
        "            state_dict = checkpoint[\"state_dict\"]\n",
        "            epoch = checkpoint[\"epoch\"]\n",
        "\n",
        "            # Ignore fixed token vectors\n",
        "            if \"token_prefix\" in state_dict:\n",
        "                del state_dict[\"token_prefix\"]\n",
        "\n",
        "            if \"token_suffix\" in state_dict:\n",
        "                del state_dict[\"token_suffix\"]\n",
        "\n",
        "            print(\"Loading weights to {} \" 'from \"{}\" (epoch = {})'.format(name, model_path, epoch))\n",
        "            # set strict=False\n",
        "            self._models[name].load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    def after_train(self):\n",
        "      print(\"Finish training\")\n",
        "\n",
        "      do_test = not self.cfg.TEST.NO_TEST\n",
        "      if do_test:\n",
        "          if self.cfg.TEST.FINAL_MODEL == \"best_val\":\n",
        "              print(\"Deploy the model with the best val performance\")\n",
        "              self.load_model(self.output_dir)\n",
        "          else:\n",
        "              print(\"Deploy the last-epoch model\")\n",
        "          acc = self.test()\n",
        "\n",
        "      # Show elapsed time\n",
        "      elapsed = round(time.time() - self.time_start)\n",
        "      elapsed = str(datetime.timedelta(seconds=elapsed))\n",
        "      print(f\"Elapsed: {elapsed}\")\n",
        "\n",
        "      # Close writer\n",
        "      self.close_writer()\n",
        "      return acc\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Generic training loops.\"\"\"\n",
        "        self.before_train()\n",
        "        for self.epoch in range(self.start_epoch, self.max_epoch):\n",
        "            self.before_epoch()\n",
        "            self.run_epoch()\n",
        "            self.after_epoch()\n",
        "        acc = self.after_train()\n",
        "        return acc"
      ],
      "metadata": {
        "id": "ueUiBs_AzMMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    cfg = setup_cfg(args)\n",
        "    if cfg.SEED >= 0:\n",
        "        set_random_seed(cfg.SEED)\n",
        "\n",
        "    if torch.cuda.is_available() and cfg.USE_CUDA:\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    trainer = build_trainer(cfg)\n",
        "    acc = trainer.train()\n",
        "    return acc"
      ],
      "metadata": {
        "id": "5JvkxdV4zKjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training CoOpCLIP on Base Class\n",
        "- The **Base class** refers to classes that were seen during training.\n",
        "- In contrast, the **New class** refers to classes that were not seen during training."
      ],
      "metadata": {
        "id": "Z51vi3Ws_hFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args.trainer = \"CoOp\"\n",
        "args.train_batch_size = 4\n",
        "args.epoch = 100\n",
        "args.output_dir = \"outputs/coop\"\n",
        "\n",
        "# Train on the Base Classes Train split and evaluate accuracy on the Base Classes Test split.\n",
        "args.subsample_classes = \"base\"\n",
        "coop_base_acc = main(args)"
      ],
      "metadata": {
        "id": "9M4KiMVlmrM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate CoOpCLIP on New Class"
      ],
      "metadata": {
        "id": "yETA2Zp6Expc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New Classes 에 대한 Acc\n",
        "args.model_dir = \"outputs/coop\"\n",
        "args.output_dir = \"outputs/coop/new_classes\"\n",
        "args.subsample_classes = \"new\"\n",
        "args.load_epoch = 100\n",
        "args.eval_only = True\n",
        "coop_novel_acc = main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB6gek8o_zQv",
        "outputId": "ff569ad5-3b7f-4a8f-96c1-c1d9bfb94a77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
            "dataset_config_file: configs/datasets/eurosat.yaml\n",
            "epoch: 100\n",
            "eval_only: True\n",
            "load_epoch: 100\n",
            "model_dir: outputs/coop\n",
            "output_dir: outputs/coop/new_classes\n",
            "root: data/\n",
            "seed: 1\n",
            "subsample_classes: new\n",
            "train_batch_size: 4\n",
            "trainer: CoOp\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 8\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 100\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 4\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: EuroSAT\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: data/\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: new\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-B/16\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.0025\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 100\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OPTIM_VNET:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.00025\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 10\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-06\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: outputs/coop/new_classes\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 20\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  COCOOP:\n",
            "    CTX_INIT: a photo of a\n",
            "    N_CTX: 4\n",
            "    PREC: fp16\n",
            "  COOP:\n",
            "    CLASS_TOKEN_POSITION: end\n",
            "    CSC: False\n",
            "    CTX_INIT: \n",
            "    N_CTX: 16\n",
            "    PREC: fp16\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: CoOp\n",
            "  PROMETAR:\n",
            "    ADAPT_LR: 0.0005\n",
            "    CTX_INIT: a photo of a\n",
            "    DIM_RATE: 8\n",
            "    FAST_ADAPTATION: False\n",
            "    LR_RATIO: 0.0005\n",
            "    MIXUP_ALPHA: 0.5\n",
            "    MIXUP_BETA: 0.5\n",
            "    N_CTX_TEXT: 4\n",
            "    N_CTX_VISION: 4\n",
            "    PREC: fp16\n",
            "    PROMPT_DEPTH_TEXT: 9\n",
            "    PROMPT_DEPTH_VISION: 9\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.3.1+cu121\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.1\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Clang version: 14.0.0-1ubuntu1.1\n",
            "CMake version: version 3.30.2\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.2.140\n",
            "CUDA_MODULE_LOADING set to: LAZY\n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 535.104.05\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                         x86_64\n",
            "CPU op-mode(s):                       32-bit, 64-bit\n",
            "Address sizes:                        46 bits physical, 48 bits virtual\n",
            "Byte Order:                           Little Endian\n",
            "CPU(s):                               2\n",
            "On-line CPU(s) list:                  0,1\n",
            "Vendor ID:                            GenuineIntel\n",
            "Model name:                           Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                           6\n",
            "Model:                                85\n",
            "Thread(s) per core:                   2\n",
            "Core(s) per socket:                   1\n",
            "Socket(s):                            1\n",
            "Stepping:                             3\n",
            "BogoMIPS:                             4000.39\n",
            "Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                    KVM\n",
            "Virtualization type:                  full\n",
            "L1d cache:                            32 KiB (1 instance)\n",
            "L1i cache:                            32 KiB (1 instance)\n",
            "L2 cache:                             1 MiB (1 instance)\n",
            "L3 cache:                             38.5 MiB (1 instance)\n",
            "NUMA node(s):                         1\n",
            "NUMA node0 CPU(s):                    0,1\n",
            "Vulnerability Gather data sampling:   Not affected\n",
            "Vulnerability Itlb multihit:          Not affected\n",
            "Vulnerability L1tf:                   Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                    Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:               Vulnerable\n",
            "Vulnerability Mmio stale data:        Vulnerable\n",
            "Vulnerability Reg file data sampling: Not affected\n",
            "Vulnerability Retbleed:               Vulnerable\n",
            "Vulnerability Spec rstack overflow:   Not affected\n",
            "Vulnerability Spec store bypass:      Vulnerable\n",
            "Vulnerability Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable (Syscall hardening enabled)\n",
            "Vulnerability Srbds:                  Not affected\n",
            "Vulnerability Tsx async abort:        Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] optree==0.12.1\n",
            "[pip3] torch==2.3.1+cu121\n",
            "[pip3] torchaudio==2.3.1+cu121\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchtext==0.18.0\n",
            "[pip3] torchvision==0.18.1+cu121\n",
            "[pip3] triton==2.3.1\n",
            "[conda] Could not collect\n",
            "        Pillow (9.4.0)\n",
            "\n",
            "Loading trainer: CoOp\n",
            "Loading dataset: EuroSAT\n",
            "Reading split from /content/ProMetaR/data/eurosat/split_zhou_EuroSAT.json\n",
            "Loading preprocessed few-shot data from /content/ProMetaR/data/eurosat/split_fewshot/shot_16-seed_1.pkl\n",
            "SUBSAMPLE NEW CLASSES!\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------\n",
            "Dataset    EuroSAT\n",
            "# classes  5\n",
            "# train_x  80\n",
            "# val      20\n",
            "# test     3,900\n",
            "---------  -------\n",
            "Loading CLIP (backbone: ViT-B/16)\n",
            "Building custom CLIP\n",
            "Initializing a generic context\n",
            "Initial context: \"X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 16\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Loading evaluator: Classification\n",
            "Loading weights to prompt_learner from \"outputs/coop/prompt_learner/model.pth.tar-100\" (epoch = 100)\n",
            "Evaluate on the *test* set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/39 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "100%|██████████| 39/39 [00:19<00:00,  3.00it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "100%|██████████| 39/39 [00:20<00:00,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> result\n",
            "* total: 3,900\n",
            "* correct: 2,007\n",
            "* accuracy: 51.5%\n",
            "* error: 48.5%\n",
            "* macro_f1: 45.6%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CoOp, CoCoOp 비교하기"
      ],
      "metadata": {
        "id": "BUuWfLBWF8UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 메트릭 데이터\n",
        "metrics = ['Base', 'Novel']\n",
        "\n",
        "coop_acc_list = [coop_base_acc, coop_novel_acc]\n",
        "cocoop_acc_list = [cocoop_base_acc, cocoop_novel_acc]\n",
        "\n",
        "\n",
        "# 막대 너비\n",
        "bar_width = 0.35\n",
        "\n",
        "# X축 위치 설정\n",
        "index = np.arange(len(metrics))\n",
        "\n",
        "# bar plot 생성\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "bar1 = ax.bar(index, coop_acc_list, bar_width, label='CoOp')\n",
        "bar2 = ax.bar(index + bar_width, cocoop_acc_list, bar_width, label='CoCoOp')\n",
        "\n",
        "# 제목과 레이블 설정\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Model Performance Comparison')\n",
        "ax.set_xticks(index + bar_width / 2)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "# 막대에 값 표시\n",
        "def add_value_labels(bars):\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 2),  # 2 points vertical offset\n",
        "                    textcoords='offset points',\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "add_value_labels(bar1)\n",
        "add_value_labels(bar2)\n",
        "\n",
        "# 그래프 출력\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "pZQxA8kiBdsj",
        "outputId": "678b3454-3107-4913-ef69-7812a2394fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}